// AUTO-GENERATED by gen.py — do not hand-edit
// Target: B200  Batch: 592  GEMM: [116032,768]×[768,768]^T
// Pipeline: 3-stage  K-iters: 6  MMA/iter: 4  idesc: 0x08200010
// Warps: 4 (128 threads)  cta_group::1
// Warp-specialized: Load(W0) | MMA(W1,cta_group::1) | Epilogue(W0-3)
// tcgen05.mma.cta_group::1.kind::f8f6f4  (E4M3 × E4M3 → FP32)

#include <cooperative_groups.h>
#include <cuda.h>
#include <curand.h>
#include <cstdint>
#include <cstdio>

#define SM_COUNT       148
#define THREADS        128
#define BATCH_SIZE     592
#define SEQ_LEN        196
#define M_TOTAL        116032
#define N_DIM          768
#define K_DIM          768
#define TM             128
#define TN             128
#define TK             128
#define TILES_M        907
#define TILES_N        6
#define K_ITERS        6
#define TOTAL_TILES    5442
#define N_STAGES       3
#define OFF_TMEM_0     98304
#define OFF_TMEM_1     98308
#define OFF_TMA_MBAR   98320
#define OFF_MMA_MBAR   98344
#define SMEM_BYTES     98368
#define TMEM_COLS      128
#define IDESC          0x08200010U
#define SBO            1024
#define TMA_BYTES      32768
#define MMA_K          32
#define MMA_PER_KI     4

#define CUDA_CHECK(x) do { \
    cudaError_t e_ = (x); \
    if (e_ != cudaSuccess) { \
        fprintf(stderr, "CUDA %s:%d: %s\n", __FILE__, __LINE__, cudaGetErrorString(e_)); \
        exit(1); \
    } \
} while(0)

#define CU_CHECK(x) do { \
    CUresult r_ = (x); \
    if (r_ != CUDA_SUCCESS) { \
        const char* s_; cuGetErrorString(r_, &s_); \
        fprintf(stderr, "CU %s:%d: %s\n", __FILE__, __LINE__, s_); \
        exit(1); \
    } \
} while(0)

// ── Device helpers ──────────────────────────────────────────

static __device__ __forceinline__
uint32_t smem_to_uint(const void* p) {
    uint32_t r;
    asm volatile("{ .reg .u64 t; cvta.to.shared.u64 t, %1; cvt.u32.u64 %0, t; }"
        : "=r"(r) : "l"(p));
    return r;
}

static __device__ __forceinline__
uint64_t make_smem_desc(uint32_t addr, uint32_t sbo) {
    uint64_t d = 0;
    d |= (uint64_t)((addr & 0x3FFFF) >> 4);
    d |= (uint64_t)((sbo  & 0x3FFFF) >> 4) << 32;
    d |= (1ULL << 46);
    d |= (1ULL << 62);  // SWIZZLE_128B
    return d;
}

static __device__ __forceinline__
void mbar_init(uint32_t addr, uint32_t count) {
    asm volatile("mbarrier.init.shared::cta.b64 [%0], %1;"
        :: "r"(addr), "r"(count));
    asm volatile("fence.mbarrier_init.release.cluster;" ::: "memory");
}

static __device__ __forceinline__
void mbar_wait(uint32_t addr, uint32_t phase) {
    uint32_t done;
    do {
        asm volatile(
            "{\n\t"
            ".reg .pred p;\n\t"
            "mbarrier.try_wait.parity.acquire.cta.shared::cta.b64 p, [%1], %2, 0x989680;\n\t"
            "selp.b32 %0, 1, 0, p;\n\t"
            "}"
            : "=r"(done) : "r"(addr), "r"(phase));
    } while (!done);
}

static __device__ __forceinline__
void mbar_arrive_expect_tx(uint32_t addr, uint32_t tx_count) {
    asm volatile(
        "mbarrier.arrive.expect_tx.release.cta.shared::cta.b64 _, [%0], %1;"
        :: "r"(addr), "r"(tx_count) : "memory");
}

static __device__ __forceinline__
void tma_load_2d(uint32_t smem_dst, const void* tma_desc,
                  int32_t c0, int32_t c1, uint32_t mbar) {
    asm volatile(
        "cp.async.bulk.tensor.2d.shared::cta.global.tile.mbarrier::complete_tx::bytes"
        " [%0], [%1, {%2, %3}], [%4];"
        :: "r"(smem_dst), "l"(tma_desc), "r"(c0), "r"(c1), "r"(mbar)
        : "memory");
}

// ═════════════════════════════════════════════════════════════
// Patch embed GEMM — warp-specialized tcgen05 (cta_group::1)
// ═════════════════════════════════════════════════════════════

__global__ void __launch_bounds__(THREADS, 1)
patch_embed_gemm(
    const __grid_constant__ CUtensorMap tma_a,
    const __grid_constant__ CUtensorMap tma_b,
    const float*   __restrict__ bias,
    const float*   __restrict__ pos_embed,
    float*         __restrict__ C
) {
    namespace cg = cooperative_groups;
    cg::grid_group grid = cg::this_grid();

    extern __shared__ __align__(128) char smem[];
    const int sm_id = blockIdx.x;
    const int tid   = threadIdx.x;
    const int warp  = tid / 32;
    const int lane  = tid % 32;

    // Per-stage SMEM offsets (codegen constants)
    static constexpr int off_a[N_STAGES] = {0, 32768, 65536};
    static constexpr int off_b[N_STAGES] = {16384, 49152, 81920};

    // ── TMEM allocation: 2 buffers (cta_group::1) ──
    // Warp 0 (WG0 leader) allocates both; all others relinquish twice
    if (warp == 0) {
        asm volatile("tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [%0], %1;"
            :: "r"(smem_to_uint(smem + OFF_TMEM_0)), "r"(TMEM_COLS));
        asm volatile("tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [%0], %1;"
            :: "r"(smem_to_uint(smem + OFF_TMEM_1)), "r"(TMEM_COLS));
    } else {
        asm volatile("tcgen05.relinquish_alloc_permit.cta_group::1.sync.aligned;");
        asm volatile("tcgen05.relinquish_alloc_permit.cta_group::1.sync.aligned;");
    }
    __syncthreads();
    const uint32_t tmem_base0 = *(volatile uint32_t*)(smem + OFF_TMEM_0);
    const uint32_t tmem_base1 = *(volatile uint32_t*)(smem + OFF_TMEM_1);
    const uint32_t tmem_base[2] = {tmem_base0, tmem_base1};

    // ── Mbarrier init ──
    if (tid == 0) {
        for (int s = 0; s < N_STAGES; s++) {
            mbar_init(smem_to_uint(smem + OFF_TMA_MBAR + s * 8), 1);
            mbar_init(smem_to_uint(smem + OFF_MMA_MBAR + s * 8), 1);
        }
    }
    __syncthreads();

    uint32_t tma_mbar[N_STAGES], mma_mbar[N_STAGES];
    uint32_t smem_a[N_STAGES], smem_b[N_STAGES];
    for (int s = 0; s < N_STAGES; s++) {
        tma_mbar[s] = smem_to_uint(smem + OFF_TMA_MBAR + s * 8);
        mma_mbar[s] = smem_to_uint(smem + OFF_MMA_MBAR + s * 8);
        smem_a[s]   = smem_to_uint(smem + off_a[s]);
        smem_b[s]   = smem_to_uint(smem + off_b[s]);
    }

    const int tile_start = (int)((long long)sm_id * TOTAL_TILES / SM_COUNT);
    const int tile_end   = (int)((long long)(sm_id + 1) * TOTAL_TILES / SM_COUNT);

    int tma_phase[N_STAGES] = {0};
    int mma_phase[N_STAGES] = {0};
    int first_tile = 1;

    for (int tile_idx = tile_start; tile_idx < tile_end; tile_idx++) {
        const int buf = tile_idx & 1;
        const int tm = tile_idx / TILES_N;
        int tn = tile_idx % TILES_N;
        if (tm & 1) tn = TILES_N - 1 - tn;  // snake: reverse odd M-rows
        const int m_start = tm * TM;
        const int n_start = tn * TN;

        // ═══ K-LOOP (W0-1) + OVERLAPPED EPILOGUE (W2-3) ═══
        if (warp == 0) {
            // ── LOAD WARP (W0): TMA async bulk copies ──
            if (lane == 0) {
                for (int ki = 0; ki < K_ITERS; ki++) {
                    const int s = ki % N_STAGES;
                    const int k_start = ki * TK;

                    if (!(first_tile && ki < N_STAGES)) {
                        mbar_wait(mma_mbar[s], mma_phase[s]);
                        mma_phase[s] ^= 1;
                    }

                    mbar_arrive_expect_tx(tma_mbar[s], TMA_BYTES);
                    tma_load_2d(smem_a[s], &tma_a, k_start, m_start, tma_mbar[s]);
                    tma_load_2d(smem_b[s], &tma_b, k_start, n_start, tma_mbar[s]);
                }
            }
        } else if (warp == 1) {
            // ── MMA WARP (W1): tcgen05.mma.cta_group::1 → tmem_base[buf] ──
            if (lane == 0) {
                for (int ki = 0; ki < K_ITERS; ki++) {
                    const int s = ki % N_STAGES;

                    mbar_wait(tma_mbar[s], tma_phase[s]);
                    tma_phase[s] ^= 1;

                    for (int sub = 0; sub < MMA_PER_KI; sub++) {
                        uint64_t desc_a = make_smem_desc(smem_a[s] + sub * MMA_K, SBO);
                        uint64_t desc_b = make_smem_desc(smem_b[s] + sub * MMA_K, SBO);
                        uint32_t accumulate = (ki == 0 && sub == 0) ? 0 : 1;

                        // MMA: cta_group::1, 4-reg enable mask
                        asm volatile(
                            "{\n\t"
                            ".reg .pred p;\n\t"
                            "setp.ne.b32 p, %4, 0;\n\t"
                            "tcgen05.mma.cta_group::1.kind::f8f6f4 "
                            "[%0], %1, %2, %3, {%5, %6, %7, %8}, p;\n\t"
                            "}"
                            :
                            : "r"(tmem_base[buf]), "l"(desc_a), "l"(desc_b), "r"(IDESC),
                              "r"(accumulate),
                              "r"(0), "r"(0), "r"(0), "r"(0));
                    }

                    asm volatile(
                        "tcgen05.commit.cta_group::1.mbarrier::arrive::one.shared::cluster.b64 [%0];"
                        :: "r"(mma_mbar[s]) : "memory");
                }
            }
        } else {
            // ── OVERLAPPED EPILOGUE (W2-3): previous tile from tmem_base[buf^1] ──
            if (!first_tile) {
                const int prev_idx = tile_idx - 1;
                const int ptm = prev_idx / TILES_N;
                int ptn = prev_idx % TILES_N;
                if (ptm & 1) ptn = TILES_N - 1 - ptn;  // snake
                const int prev_m = ptm * TM;
                const int prev_n = ptn * TN;
                const uint32_t prev_tmem = tmem_base[buf ^ 1];

                for (int pass = 0; pass < 2; pass++) {
                    const int ew = (warp - 2) + pass * 2;  // 0,1,2,3
                    for (int nc = 0; nc < TN; nc += 8) {
                        float v0, v1, v2, v3, v4, v5, v6, v7;
                        int addr = prev_tmem + (ew * 32 << 16) + nc;

                        asm volatile(
                            "tcgen05.ld.sync.aligned.32x32b.x8.b32 "
                            "{%0, %1, %2, %3, %4, %5, %6, %7}, [%8];"
                            : "=f"(v0), "=f"(v1), "=f"(v2), "=f"(v3),
                              "=f"(v4), "=f"(v5), "=f"(v6), "=f"(v7)
                            : "r"(addr));
                        asm volatile("tcgen05.wait::ld.sync.aligned;" ::: "memory");

                        int gm = prev_m + ew * 32 + lane;
                        int gn = prev_n + nc;
                        if (gm < M_TOTAL) {
                            int pos_row = gm % SEQ_LEN;
                            float* out  = C + (long long)gm * N_DIM;
                            const float* b = bias;
                            const float* p = pos_embed + (long long)pos_row * N_DIM;
                            if (gn + 0 < N_DIM) out[gn+0] = v0 + b[gn+0] + p[gn+0];
                            if (gn + 1 < N_DIM) out[gn+1] = v1 + b[gn+1] + p[gn+1];
                            if (gn + 2 < N_DIM) out[gn+2] = v2 + b[gn+2] + p[gn+2];
                            if (gn + 3 < N_DIM) out[gn+3] = v3 + b[gn+3] + p[gn+3];
                            if (gn + 4 < N_DIM) out[gn+4] = v4 + b[gn+4] + p[gn+4];
                            if (gn + 5 < N_DIM) out[gn+5] = v5 + b[gn+5] + p[gn+5];
                            if (gn + 6 < N_DIM) out[gn+6] = v6 + b[gn+6] + p[gn+6];
                            if (gn + 7 < N_DIM) out[gn+7] = v7 + b[gn+7] + p[gn+7];
                        }
                    }
                }
            }
        }

        // ── Fence + sync: drain MMA writes + block until epilogue done ──
        asm volatile("tcgen05.fence::before_thread_sync;" ::: "memory");
        __syncthreads();
        asm volatile("tcgen05.fence::after_thread_sync;" ::: "memory");
        first_tile = 0;
    }  // tile loop

    // ── Drain epilogue: all 4 warps, last tile ──
    if (tile_start < tile_end) {
        const int last_idx = tile_end - 1;
        const int last_buf = last_idx & 1;
        const int ltm = last_idx / TILES_N;
        int ltn = last_idx % TILES_N;
        if (ltm & 1) ltn = TILES_N - 1 - ltn;  // snake
        const int last_m = ltm * TM;
        const int last_n = ltn * TN;
        const uint32_t drain_tmem = tmem_base[last_buf];

        for (int nc = 0; nc < TN; nc += 8) {
            float v0, v1, v2, v3, v4, v5, v6, v7;
            int addr = drain_tmem + (warp * 32 << 16) + nc;

            asm volatile(
                "tcgen05.ld.sync.aligned.32x32b.x8.b32 "
                "{%0, %1, %2, %3, %4, %5, %6, %7}, [%8];"
                : "=f"(v0), "=f"(v1), "=f"(v2), "=f"(v3),
                  "=f"(v4), "=f"(v5), "=f"(v6), "=f"(v7)
                : "r"(addr));
            asm volatile("tcgen05.wait::ld.sync.aligned;" ::: "memory");

            int gm = last_m + warp * 32 + lane;
            int gn = last_n + nc;
            if (gm < M_TOTAL) {
                int pos_row = gm % SEQ_LEN;
                float* out  = C + (long long)gm * N_DIM;
                const float* b = bias;
                const float* p = pos_embed + (long long)pos_row * N_DIM;
                if (gn + 0 < N_DIM) out[gn+0] = v0 + b[gn+0] + p[gn+0];
                if (gn + 1 < N_DIM) out[gn+1] = v1 + b[gn+1] + p[gn+1];
                if (gn + 2 < N_DIM) out[gn+2] = v2 + b[gn+2] + p[gn+2];
                if (gn + 3 < N_DIM) out[gn+3] = v3 + b[gn+3] + p[gn+3];
                if (gn + 4 < N_DIM) out[gn+4] = v4 + b[gn+4] + p[gn+4];
                if (gn + 5 < N_DIM) out[gn+5] = v5 + b[gn+5] + p[gn+5];
                if (gn + 6 < N_DIM) out[gn+6] = v6 + b[gn+6] + p[gn+6];
                if (gn + 7 < N_DIM) out[gn+7] = v7 + b[gn+7] + p[gn+7];
            }
        }
    }

    __syncthreads();  // all warps done before dealloc

    grid.sync();

    // ── TMEM dealloc: 2 buffers (warp 0, cta_group::1) ──
    if (warp == 0) {
        asm volatile("tcgen05.dealloc.cta_group::1.sync.aligned.b32 %0, %1;"
            :: "r"(tmem_base[0]), "r"(TMEM_COLS));
        asm volatile("tcgen05.dealloc.cta_group::1.sync.aligned.b32 %0, %1;"
            :: "r"(tmem_base[1]), "r"(TMEM_COLS));
    }
}

// ═════════════════════════════════════════════════════════════
// Host
// ═════════════════════════════════════════════════════════════

int main() {
    printf("SigLIP2 patch embed GEMM — tcgen05 cta_group::1 (4 warps)\n");
    printf("  GEMM: [%d,%d] x [%d,%d]^T  3-stage pipeline  idesc: 0x%08X\n",
           M_TOTAL, K_DIM, N_DIM, K_DIM, IDESC);

    uint8_t *d_A, *d_B;
    float *d_bias, *d_pos, *d_C;
    CUDA_CHECK(cudaMalloc(&d_A,    (size_t)M_TOTAL * K_DIM));
    CUDA_CHECK(cudaMalloc(&d_B,    (size_t)N_DIM   * K_DIM));
    CUDA_CHECK(cudaMalloc(&d_bias,  (size_t)N_DIM  * sizeof(float)));
    CUDA_CHECK(cudaMalloc(&d_pos,   (size_t)SEQ_LEN * N_DIM * sizeof(float)));
    CUDA_CHECK(cudaMalloc(&d_C,    (size_t)M_TOTAL * N_DIM * sizeof(float)));

    curandGenerator_t rng;
    curandCreateGenerator(&rng, CURAND_RNG_PSEUDO_DEFAULT);
    curandSetPseudoRandomGeneratorSeed(rng, 42);
    curandGenerate(rng, (unsigned int*)d_A, ((size_t)M_TOTAL * K_DIM + 3) / 4);
    curandGenerate(rng, (unsigned int*)d_B, ((size_t)N_DIM * K_DIM + 3) / 4);
    curandGenerateUniform(rng, d_bias, N_DIM);
    curandGenerateUniform(rng, d_pos,  SEQ_LEN * N_DIM);
    curandDestroyGenerator(rng);

    CUtensorMap h_tma_a, h_tma_b;

    {
        uint64_t dims[2]    = {K_DIM, M_TOTAL};
        uint64_t strides[1] = {(uint64_t)K_DIM};
        uint32_t box[2]     = {TK, TM};
        uint32_t estrides[2]= {1, 1};
        CU_CHECK(cuTensorMapEncodeTiled(&h_tma_a,
            CU_TENSOR_MAP_DATA_TYPE_UINT8, 2, (void*)d_A,
            dims, strides, box, estrides,
            CU_TENSOR_MAP_INTERLEAVE_NONE,
            CU_TENSOR_MAP_SWIZZLE_128B,
            CU_TENSOR_MAP_L2_PROMOTION_NONE,
            CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE));
    }

    {
        uint64_t dims[2]    = {K_DIM, N_DIM};
        uint64_t strides[1] = {(uint64_t)K_DIM};
        uint32_t box[2]     = {TK, TN};
        uint32_t estrides[2]= {1, 1};
        CU_CHECK(cuTensorMapEncodeTiled(&h_tma_b,
            CU_TENSOR_MAP_DATA_TYPE_UINT8, 2, (void*)d_B,
            dims, strides, box, estrides,
            CU_TENSOR_MAP_INTERLEAVE_NONE,
            CU_TENSOR_MAP_SWIZZLE_128B,
            CU_TENSOR_MAP_L2_PROMOTION_NONE,
            CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE));
    }

    CUDA_CHECK(cudaFuncSetAttribute(patch_embed_gemm,
        cudaFuncAttributeMaxDynamicSharedMemorySize, SMEM_BYTES));

    void* args[] = { &h_tma_a, &h_tma_b, &d_bias, &d_pos, &d_C };
    CUDA_CHECK(cudaLaunchCooperativeKernel(
        (void*)patch_embed_gemm,
        dim3(SM_COUNT), dim3(THREADS), args, SMEM_BYTES));

    CUDA_CHECK(cudaDeviceSynchronize());
    printf("Kernel completed.\n");

    float* h_C = (float*)malloc((size_t)M_TOTAL * N_DIM * sizeof(float));
    CUDA_CHECK(cudaMemcpy(h_C, d_C, (size_t)M_TOTAL * N_DIM * sizeof(float), cudaMemcpyDeviceToHost));

    double cksum = 0;
    for (int i = 0; i < 1024 && i < M_TOTAL * N_DIM; i++) cksum += h_C[i];
    printf("Checksum (first 1024): %f\n", cksum);
    printf("C[0,0..3] = %f %f %f %f\n", h_C[0], h_C[1], h_C[2], h_C[3]);

    free(h_C);
    cudaFree(d_A); cudaFree(d_B); cudaFree(d_bias); cudaFree(d_pos); cudaFree(d_C);
    return 0;
}