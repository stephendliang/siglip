**Optimizing a Blackwell (SM100) GEMM epilogue for a SigLIP2 patch embedding kernel.** The kernel is warp-specialized (W0=TMA loads, W1=MMA, W2-5=epilogue) doing FP8×FP8→FP32→BF16 with fused bias+positional embedding add. The GEMM itself is fast; the epilogue is the bottleneck. Currently it uses TMA stores through SMEM (requiring `tma_store_fence`, `__syncwarp`, lane-0-only store issue, and `tma_store_wait`), a `__syncthreads` that couples all 6 warps at every tile boundary, and only 4 of 6 warps doing TMEM reads — wasting 33% of available interleaving depth on the ~200-cycle `tcgen05.ld` latency.

The reference point is NVIDIA's own SUPERIOR.cu pattern (from CUTLASS v4), which already solved most of these problems: mbarrier-based decoupling instead of `__syncthreads`, `st.global.v8.b32` instead of TMA stores, and clean warp-specialized structure. The kernel currently copies the SUPERIOR mainloop structure but uses an inferior epilogue — essentially hand-rolled TMA store pipelining with double-buffered SMEM staging, when the reference code just does direct global stores. The gap is known and the fixes are largely "transplant patterns that already exist 200 lines away in the same repo."

The planned attack order is: (1) refactor duplicated epilogue into a function, (2) replace `__syncthreads` with mbarriers, (3) replace TMA stores with `st.global`, (4) delete dead SMEM/TMA-store code, (5) prefetch bias+pos into SMEM during K-loop, (6) make all 6 warps participate in the epilogue for 6-way TMEM interleaving. Steps 1-4 are mechanical. Step 5 is moderately hard due to the per-row positional embedding access pattern. Step 6 is the real challenge — staggered warp entry into the epilogue (W0 finishes TMA before W1 finishes MMA) makes the synchronization tricky.
